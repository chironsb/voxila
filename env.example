# Ollama Configuration
OLLAMA_MODEL=qwen3:1.7b
OLLAMA_BASE_URL=http://localhost:11434
# Show reasoning in responses (true/false) - if true, you'll see the model's thinking process
SHOW_REASONING=false
# Disable reasoning completely (true/false) - if true, adds STRONG instruction to model to not use reasoning
# RECOMMENDED: Set to true to avoid extremely long reasoning for simple questions
DISABLE_REASONING=true

# Whisper ASR Configuration
WHISPER_MODEL_SIZE=base.en
WHISPER_DEVICE=cpu
# WHISPER_DEVICE=cuda  # Use GPU if available

# TTS Configuration
# OpenVoice (like original project - requires checkpoints)
OPENVOICE_CHECKPOINTS=./checkpoints
OPENVOICE_REFERENCE=./voxila.mp3
OPENVOICE_DEVICE=cpu
# Use 'cuda' only if you have enough GPU memory (8GB+ recommended)
# Coqui TTS (fallback, no checkpoints needed)
COQUI_TTS_MODEL=tts_models/en/ljspeech/tacotron2-DDC
# Piper TTS (fallback, lightweight)
PIPER_BINARY_PATH=./models/piper/piper
PIPER_MODEL_PATH=./models/piper/en_US-amy-medium.onnx

# Embedding Model Configuration
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
# Alternatives: all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2

# RAG Configuration
VAULT_FILE=./data/vault.txt
RAG_TOP_K=3
FAISS_INDEX_PATH=./data/embeddings/faiss_index

# Audio Configuration
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1

